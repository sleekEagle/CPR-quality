import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import utils
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
from autodistill_grounding_dino import GroundingDINO
from autodistill.detection import CaptionOntology
from segment_anything import sam_model_registry
from segment_anything import SamAutomaticMaskGenerator
from segment_anything import SamPredictor
import torch
import cv2
import open3d as o3d
from scipy.spatial.transform import Rotation as R
from scipy.optimize import minimize


skip=120
kinect_root=r'D:\hand_depth_dataset\kinect'
canon_root=r'D:\hand_depth_dataset\canon'
#object detection model
base_model = GroundingDINO(ontology=CaptionOntology({"hand": "hand"}))

#hand segmentation model
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
MODEL_TYPE = "vit_h"
CHECKPOINT_PATH=r'C:\Users\lahir\code\CPR-quality\sam_vit_h_4b8939.pth'
sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)
sam.to(device=DEVICE)
mask_predictor = SamPredictor(sam)

def get_interp_ts_list(files):
    process_ar=[0]*len(files)
    ind_list,ts_list=[],[]
    
    for i in range(len(files)):
        if i%skip!=0:
            continue
        # ts=None
        # j=i
        # while not ts:
        #     color_file=os.path.join(color_dir,files[j])
        #     ts=utils.get_ts_google(color_file)
        #     j+=1
        #     if j==i+skip:
        #         break

        ts=utils.get_ts_google(files[i])
        if ts:
            process_ar[i]=1
            ind_list.append(i)
            ts_list.append(ts)

    ms_list=[utils.get_ms_from_ts(ts) for ts in ts_list]
    intterp_ts_list=[0]*len(files)
    for i,idx in enumerate(ind_list):
        intterp_ts_list[idx]=ms_list[i]
    #interpolate
    ind_list.sort()
    ind_list=np.array(ind_list)
    for i in range(len(intterp_ts_list)):
        if intterp_ts_list[i]==0:
            if i<=ind_list[0]:
                I1,I2=ind_list[0:2]
            elif i>=ind_list[-1]:
                I1,I2=ind_list[-2:]                
            else:
                I1=max(ind_list[ind_list<i])
                I2=min(ind_list[ind_list>i])
            T1,T2=intterp_ts_list[I1],intterp_ts_list[I2]
            t_interp=T2 + (T2-T1)/(I2-I1)*(i-I2)
            intterp_ts_list[i]=t_interp
    return intterp_ts_list


def get_ts_kinect():
    dirs=utils.list_subdirectories(kinect_root) 
    for dir in dirs:
        print(dir)
        out_file=os.path.join(kinect_root,dir,'ts.txt')
        if os.path.exists(out_file):
            print('ts file already exists. continuing...')
            continue
        color_dir=os.path.join(kinect_root,dir,'color')
        files=utils.list_files(color_dir,'jpg')
        files=[os.path.join(kinect_root,dir,'color',f) for f in files]
        
        intterp_ts_list=get_interp_ts_list(files)
        #save to file
        with open(out_file, 'w') as f:
            for i in range(len(intterp_ts_list)):
                f.write(str(files[i])+','+str(intterp_ts_list[i])+'\n')

def get_ts_canon():
    dirs=utils.list_subdirectories(canon_root)
    for dir in dirs:
        print(dir)
        out_file=os.path.join(canon_root,dir,'ts.txt')
        if os.path.exists(out_file):
            print('ts file already exists. continuing...')
            continue
        files=utils.list_files(os.path.join(canon_root,dir),'jpg')
        files=[os.path.join(canon_root,dir,f) for f in files]
        intterp_ts_list=get_interp_ts_list(files)
        #save to file
        with open(out_file, 'w') as f:
            for i in range(len(intterp_ts_list)):
                f.write(str(files[i])+','+str(intterp_ts_list[i])+'\n')

def get_ms_ts(file):
    ts=utils.get_ts_google(file)
    if ts:
        ts=utils.get_ms_from_ts(ts)
    return ts

def get_seg(file,bb):
    image = cv2.imread(file)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    area=(bb[3]-bb[1])*(bb[2]-bb[0])

    bb_=[int(b) for b in bb]

    mask_predictor = SamPredictor(sam)
    mask_predictor.set_image(image_rgb)

    if area<200000:
        input_point = np.array([[(bb_[0]+bb_[2])*0.5, (bb_[1]+bb_[3])*0.5]])
        input_label = np.array([1])
        mask, scores, logits = mask_predictor.predict(
        point_coords=input_point,
        point_labels=input_label,
        multimask_output=False,
        )
        mask=np.squeeze(mask)
        #remove mask outside the bb
        anti_mask=np.zeros_like(mask)
        anti_mask[bb_[1]:bb_[3],bb_[0]:bb_[2]]=1
        mask=mask*anti_mask
    else:
        mask_predictor.set_image(image_rgb)
        mask, scores, logits = mask_predictor.predict(
            box=np.array(bb_),
            multimask_output=False
            )
        mask=np.squeeze(mask)


    # bb_=[int(b) for b in bb]
    # pad=0
    # bb_=[bb_[0]-pad,bb_[1]-pad,bb_[2]+pad,bb_[3]+pad]
    # # img_cropped=utils.crop_img_bb(image_rgb,bb,50)
    # # utils.draw_bb(file,bb)

    # mask_predictor.set_image(image_rgb)
    # masks, scores, logits = mask_predictor.predict(
    #     box=np.array(bb_),
    #     multimask_output=True
    #     )


    mask=mask*255
    mask=mask.astype(np.uint8)

    # mask=np.any(mask, axis=0)*255
    # mask=mask.astype(np.uint8)
    # plt.imshow(mask*anti_mask)
    return mask

def get_point_cloud(file,mask=None):
    depth_file=os.path.join(file.replace('jpg','png').replace('color','depth'))
    depth_img = cv2.imread(depth_file, cv2.IMREAD_GRAYSCALE | cv2.IMREAD_ANYDEPTH)
    if mask is not None:
        depth_img=depth_img*(mask/mask.max())
    d=np.argwhere(depth_img>0)
    x,y=d[:,0],d[:,1]
    depth_valus=depth_img[x,y]
    X,Y,Z=utils.get_XYZ_kinect(x,y,depth_valus)

    # points = np.vstack((X, Y, Z)).T
    # pcd_lower = o3d.geometry.PointCloud()
    # pcd_lower.points = o3d.utility.Vector3dVector(points)
    # o3d.visualization.draw_geometries([pcd_lower], window_name="Aligned Point Clouds")


    return X,Y,Z

def get_centroid(image):
    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    # Assuming the largest contour corresponds to the object of interest
    largest_contour = max(contours, key=cv2.contourArea)
    M = cv2.moments(largest_contour)
    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return np.array([cx, cy])
            
def sync_imgs():
    dirs=utils.list_subdirectories(kinect_root)
    for dir in dirs:
        ts_path=os.path.join(kinect_root,dir,'ts.txt')
        with open(ts_path, 'r') as f:
            lines = f.readlines()
        kinect_files=[l.split(',')[0] for l in lines]
        kinect_ts=np.array([float(l.split(',')[1].strip()) for l in lines])
        
        indices = list(range(0, len(kinect_files), 30))
        selected_kinect_files=[k for i,k in enumerate(kinect_files) if i in indices]
        selected_kinect_ts_list=np.array([k for i,k in enumerate(kinect_ts) if i in indices])

        #get all canon files and ts
        part=dir.split('_')[0]
        canon_dirs=utils.get_dirs_with_str(canon_root,part,i=0,j=1)
        canon_files_list,canon_ts_list=[],[]
        for d in canon_dirs:
            canon_ts_file=os.path.join(canon_root,d,'ts.txt')
            with open(canon_ts_file, 'r') as f:
                lines = f.readlines()
            canon_files=[os.path.join(d,os.path.basename(l.split(',')[0])) for l in lines]
            canon_ts=[float(l.split(',')[1].strip()) for l in lines]
            canon_files_list.extend(canon_files)
            canon_ts_list.extend(canon_ts)

        for ind in indices:
            if ind<1000:
                continue
            ts=kinect_ts[ind]
            k_file=kinect_files[ind]
            #find the closest canon image
            closest_idx=np.argmin(np.abs(canon_ts_list-ts))
            canon_ts=canon_ts_list[closest_idx]
            closest_file=canon_files_list[closest_idx]
            #get the kinect images around this canon image
            v=kinect_ts-canon_ts
            if len(v[v<=0])==0:
                kinect_closest_lower_idx=ind
            else:
                kinect_closest_lower_idx=max(np.argwhere(v<=0))[0]
            if len(v[v>=0])==0:
                kinect_closest_upper_idx=ind
            else:
                kinect_closest_upper_idx=min(np.argwhere(v>=0))[0]

            k_lower_file=os.path.join(kinect_root,dir,'color',kinect_files[kinect_closest_lower_idx])
            k_upper_file=os.path.join(kinect_root,dir,'color',kinect_files[kinect_closest_upper_idx])
            k_file=os.path.join(kinect_root,dir,'color',k_file)
            canon_file=closest_file

            #get actual ts from images
            ts=get_ms_ts(k_file)
            k_ts = ts if ts else kinect_ts[ind]

            if k_lower_file==k_file:
                k_lower_ts=k_ts
            else:
                ts=get_ms_ts(k_lower_file)
                k_lower_ts = ts if ts else kinect_ts[kinect_closest_lower_idx]

            if k_upper_file==k_file:
                k_upper_ts=k_ts
            else:
                ts=get_ms_ts(k_upper_file)
                k_upper_ts = ts if ts else kinect_ts[kinect_closest_upper_idx]

            ts=get_ms_ts(canon_file)
            c_ts = ts if ts else canon_ts

            while k_upper_ts<=c_ts:
                kinect_closest_upper_idx+=1
                k_upper_file=os.path.join(kinect_root,dir,'color',kinect_files[kinect_closest_upper_idx])
                ts=get_ms_ts(k_upper_file)
                k_upper_ts = ts if ts else kinect_ts[kinect_closest_upper_idx]
            while k_lower_ts>c_ts:
                kinect_closest_lower_idx-=1
                k_lower_file=os.path.join(kinect_root,dir,'color',kinect_files[kinect_closest_lower_idx])
                ts=get_ms_ts(k_lower_file)
                k_lower_ts = ts if ts else kinect_ts[kinect_closest_lower_idx]

            assert k_lower_ts<=c_ts<=k_upper_ts , 'timestamps not in order'
            
            print(k_lower_ts,k_ts,k_upper_ts,c_ts)
            print('lower , upper:',kinect_closest_lower_idx,kinect_closest_upper_idx)
            
            #detect bbs
            results = base_model.predict(k_lower_file)
            lower_bb=utils.get_bb(results)
            results = base_model.predict(k_upper_file)
            upper_bb=utils.get_bb(results)
            results = base_model.predict(closest_file)
            canon_bb=utils.get_bb(results)

            # utils.draw_bb(closest_file,canon_bb)

            if (len(lower_bb)*len(upper_bb)*len(canon_bb))==0:
                print('no hand detected')
                continue
                
            #detect hand segmentation
            lower_mask=get_seg(k_lower_file,lower_bb)
            upper_mask=get_seg(k_upper_file,upper_bb)
            canon_mask=get_seg(closest_file,canon_bb)

            # utils.draw_bb(closest_file,canon_bb)

            # plt.imshow(upper_mask)

            # Read the image
            X_lower,Y_lower,Z_lower=get_point_cloud(k_lower_file,lower_mask)
            X_upper,Y_upper,Z_upper=get_point_cloud(k_upper_file,upper_mask)

            points = np.vstack((X_lower, Y_lower, Z_lower)).T
            pcd_lower = o3d.geometry.PointCloud()
            pcd_lower.points = o3d.utility.Vector3dVector(points)

            # X,Y,Z=np.array(pcd_lower.points).T
            # x,y=utils.project_3d_to_2d(X,Y,Z,utils.kinect_k)
            # kinect_img=np.zeros_like(lower_mask)
            # for i in range(len(x)):
            #     kinect_img[int(x[i]),int(y[i])]=1
            # plt.imshow(kinect_img)


            points = np.vstack((X_upper, Y_upper, Z_upper)).T
            pcd_upper = o3d.geometry.PointCloud()
            pcd_upper.points = o3d.utility.Vector3dVector(points)

            init_transformation = np.eye(4)
            icp_result = o3d.pipelines.registration.registration_icp(
            pcd_lower, pcd_upper, max_correspondence_distance=0.02,  # Set according to your data scale
            init=init_transformation,
            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(),
            criteria=o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=2000)
            )
            transformation_matrix = icp_result.transformation
            print("Transformation Matrix:")
            print(transformation_matrix)
            
            # pcd_lower.transform(transformation_matrix)
            # o3d.visualization.draw_geometries([pcd_lower, pcd_upper], window_name="Aligned Point Clouds")


            frac=(c_ts-k_lower_ts)/(k_upper_ts-k_lower_ts)
            assert frac>0, 'fraction should be positive'

            Rot=transformation_matrix[:3,:3]
            T=transformation_matrix[:3,3]

            T_new=T*frac

            r = R.from_matrix(Rot.copy())
            euler_angles = r.as_euler('xyz', degrees=True)
            new_angles=euler_angles*frac
            new_rot=R.from_euler('xyz', new_angles, degrees=True).as_matrix()

            T_int = np.eye(4)
            T_int[:3,:3]=new_rot
            T_int[:3,3]=T_new
            pcd_lower.transform(T_int)
            #transform into canon coordinate frame
            pcd_lower.transform(utils.kinect_to_canon)
            #project the point cloud into the canon image
            X,Y,Z=np.array(pcd_lower.points).T

            # points = np.vstack((X, Y, Z)).T
            # pcd_lower = o3d.geometry.PointCloud()
            # pcd_lower.points = o3d.utility.Vector3dVector(points)
            # o3d.visualization.draw_geometries([pcd_lower], window_name="Aligned Point Clouds")


            x,y=utils.project_3d_to_2d(X,Y,Z,utils.canon_k)
            canon_proj_thres=np.zeros(utils.canon_original_res)
            for i in range(len(x)):
                if int(x[i])<0 or int(y[i])<0 or int(y[i])>lower_mask.shape[1]-1 or int(x[i])>lower_mask.shape[0]-1:
                    continue
                canon_proj_thres[int(x[i]),int(y[i])]=1
            # canon_proj_thres=canon_proj_thres.astype(np.uint8)
            # density_map_blurred = cv2.GaussianBlur(canon_proj_thres, (0, 0), sigmaX=2, sigmaY=2)
            # density_map_blurred[density_map_blurred>0]=1
            canon_proj_thres=canon_proj_thres.astype(np.uint8)
            h,w=canon_mask.shape
            h_,w_=canon_proj_thres.shape
            # Pad the density map to match the size of the canon mask
            pad_h = (h - h_)
            pad_w = (w - w_)
            canon_proj_thres = np.pad(canon_proj_thres, ((0, pad_h), (0, pad_w)), mode='constant', constant_values=0)

            # scale  and roughly align images

            def opt_scale(source_img,target_img):
                x1,y1=np.where(source_img)
                xlen1,ylen1=x1.max()-x1.min(),y1.max()-y1.min()
                x2,y2=np.where(target_img)
                xlen2,ylen2=x2.max()-x2.min(),y2.max()-y2.min()
                x_scale=xlen1/xlen2
                y_scale=ylen1/ylen2
                cropped_img=target_img[x2.min():x2.max(),y2.min():y2.max()]
                cropped_img=cv2.resize(cropped_img,(int(ylen2*y_scale),int(xlen2*x_scale)))
                aligned_img=np.zeros_like(source_img)
                aligned_img[x1.max()-cropped_img.shape[0]:x1.max(),y1.min():y1.min()+cropped_img.shape[1]]=cropped_img
                return aligned_img, (x_scale+y_scale)*0.5
            
            def opt_rotate(source_img,target_img):
                from scipy.optimize import minimize
                from skimage.transform import rotate

                def cost_function(angle, image_to_rotate, reference_image):
                    rotated_image = rotate(image_to_rotate, angle[0], resize=False, mode='edge')
                    difference = np.abs(reference_image - rotated_image)
                    cost = np.sum(difference)
                    return cost
                initial_guess = [0]
                result = minimize(cost_function, initial_guess, args=(target_img, source_img), method='Powell')
                best_angle = result.x
                aligned_image = rotate(target_img, best_angle[0], resize=False, mode='edge')
                # utils.show_img_overlay(target_img,aligned_image)
                return aligned_image,best_angle
            
            #aligne by translation
            def opt_trans(source_img,target_img):
                source_img[source_img>0]=1
                target_img[target_img>0]=1
                x1,y1=np.where(source_img)
                xlen1,ylen1=x1.max()-x1.min(),y1.max()-y1.min()
                x2,y2=np.where(target_img)
                xlen2,ylen2=x2.max()-x2.min(),y2.max()-y2.min()
                aligned_image=np.zeros_like(source_img)
                xmin=max(0,x1.max()-xlen2)
                xlen=x1.max()-xmin
                ymax=min(source_img.shape[1],y1.min()+ylen2)
                ylen=ymax-y1.min()
                aligned_image[xmin:x1.max(),y1.min():ymax]=target_img[x2.min():x2.min()+xlen,y2.min():y2.min()+ylen]
                x_move=abs(x2.max()-x1.max())
                y_move=abs(y2.min()-y1.min())
                return aligned_image,x_move,y_move


            aligned_img=canon_proj_thres
            for i in range(10):
                aligned_img,scale=opt_scale(canon_mask,aligned_img)
                print('scale: ',scale)
                aligned_img,angle=opt_rotate(canon_mask,aligned_img)
                print('angle: ',angle)
                aligned_img,x_move,y_move=opt_trans(canon_mask,aligned_img)
                print('movement: ',(x_move,y_move))
            utils.show_img_overlay(canon_mask,aligned_img)

            # X,Y,Z=np.array(pcd_lower.points).T
            # x,y=utils.project_3d_to_2d(X,Y,Z,utils.canon_k)
            # kinect_img=np.zeros_like(lower_mask)
            # for i in range(len(x)):
            #     if int(x[i])<0 or int(y[i])<0 or int(y[i])>lower_mask.shape[1]-1 or int(x[i])>lower_mask.shape[0]-1:
            #         continue
            #     kinect_img[int(x[i]),int(y[i])]=1
            # plt.imshow(kinect_img)
            # plt.show()
            pass
            
            # o3d.visualization.draw_geometries([pcd_lower], window_name="Aligned Point Clouds")

            # utils.transform_kinect_to_canon()

            # 
            # o3d.visualization.draw_geometries([pcd_lower, pcd_upper], window_name="Aligned Point Clouds")

            # plt.imshow(lower_mask)
            # x,y=utils.project_3d_to_2d(X_lower,Y_lower,Z_lower)
            # depth_img=np.zeros_like(lower_mask)
            # for i in range(len(x)):
            #     depth_img[int(x[i]),int(y[i])]=1

            # plt.imshow(depth_img)


if __name__ == "__main__":
    sync_imgs()



# timestamp_str = "12:26:22.439"
# utils.get_ms_from_ts(timestamp_str)




